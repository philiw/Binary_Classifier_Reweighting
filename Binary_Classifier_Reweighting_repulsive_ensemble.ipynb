{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f403c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import mplhep as hep\n",
    "from comet_ml import Experiment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85623ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-factor:  0.7826885718054716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.hist(rw_weights_eft, bins=100, label=\"Reweighted weights EFT\")\\nplt.hist(weights_target, bins=100, label=\"Reweighted weights Target\")\\nplt.legend()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"C:/Users/phili/Desktop/Studium/6. Semester/Semesterarbeit/dataset\"\n",
    "\n",
    "df_target = ak.from_parquet(folder+'/ttHTobb_2018_test.parquet')\n",
    "df_eft_tbarlnutqq = ak.from_parquet(folder+'/ttHTobb_EFT_tbarlnutqq_2018_test.parquet')\n",
    "df_eft_tbarqqtlnu = ak.from_parquet(folder+'/ttHTobb_EFT_tbarqqtlnu_2018_test.parquet')\n",
    "\n",
    "df_eft = ak.concatenate([df_eft_tbarlnutqq, df_eft_tbarqqtlnu])\n",
    "\n",
    "def reweight(weights_eft, weights_target):\n",
    "    weights_eft.to_numpy(); weights_target.to_numpy()\n",
    "    \n",
    "    A = np.sum(weights_target)/np.sum(weights_eft)\n",
    "        \n",
    "    return A, A*weights_eft\n",
    "\n",
    "weights_eft = df_eft.weight\n",
    "weights_target = df_target.weight\n",
    "\n",
    "A, weights_eft = reweight(weights_eft, weights_target)\n",
    "df_eft = ak.with_field(df_eft, weights_eft, \"weight\")\n",
    "\n",
    "\n",
    "print(\"K-factor: \", A)\n",
    "\"\"\"\n",
    "plt.hist(rw_weights_eft, bins=100, label=\"Reweighted weights EFT\")\n",
    "plt.hist(weights_target, bins=100, label=\"Reweighted weights Target\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752f6fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 943341, 15]) torch.Size([20, 943341, 1]) torch.Size([20, 943341, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_feature(col: np.ndarray, transform: str = None):\n",
    "    \"\"\"\n",
    "    Apply optional transform to col, then z-score:\n",
    "      - transform=\"log1p\": replace col -> log(col + 1)\n",
    "      - transform=None: leave col as is\n",
    "    Returns the transformed, zero-mean, unit-std array.\n",
    "    \"\"\"\n",
    "    # 1) optional transform\n",
    "    if transform == \"log1p\":\n",
    "        col = np.log(col + 1)\n",
    "    # 2) compute mean & std\n",
    "    mu = np.mean(col)\n",
    "    sigma = np.std(col)\n",
    "    # 3) z-score\n",
    "    return (col - mu) / (sigma if sigma != 0 else 1.0)\n",
    "\n",
    "def build_X_and_standardize(df_combined, objects, fields, orders):\n",
    "    \"\"\"\n",
    "    Pull out raw arrays, standardize each (pt->log1p+z, eta->z, phi->z),\n",
    "    and stack into an (N, D) array.\n",
    "    \"\"\"\n",
    "    X_cols = []\n",
    "    for obj in objects:\n",
    "        for field in fields:\n",
    "            for order in orders:\n",
    "                # extract a 1d numpy array\n",
    "                raw = getattr(getattr(df_combined, obj), field)[:, order].to_numpy()\n",
    "                # pick transform type\n",
    "                if field == \"pt\":\n",
    "                    col = standardize_feature(raw, transform=\"log1p\")\n",
    "                else:\n",
    "                    col = standardize_feature(raw, transform=None)\n",
    "                X_cols.append(col)\n",
    "    # stack into (n_events, n_features)\n",
    "    return np.stack(X_cols, axis=1)\n",
    "\n",
    "# ——————————————————————————————\n",
    "# usage:\n",
    "\n",
    "n_epochs = 20  # adjust as needed\n",
    "hidden_dim = 64  # hidden layer size\n",
    "batch_size = 2048  # adjust as needed\n",
    "learning_rate = 1e-3  # adjust as needed\n",
    "channels = 20\n",
    "prior_width = 1.\n",
    "\n",
    "# 1) label & concat as before\n",
    "df_target[\"label\"] = 1\n",
    "df_eft   [\"label\"] = 0\n",
    "df_combined = ak.concatenate([df_target, df_eft])\n",
    "\n",
    "# 2) define your objects/fields/orders\n",
    "objects = [\"top\", \"antitop\", \"lepton_gen\", \"lepton_reco\", \"higgs\"]\n",
    "fields  = [\"pt\", \"eta\", \"phi\"]\n",
    "orders  = [0]\n",
    "length_features = len(objects)*len(fields)*len(orders)\n",
    "\n",
    "# 3) build standardized X\n",
    "X = build_X_and_standardize(df_combined, objects, fields, orders)\n",
    "y = df_combined.label.to_numpy()\n",
    "w = df_combined.weight.to_numpy()\n",
    "\n",
    "# 4) to torch \n",
    "X_torch = torch.tensor(X, dtype=torch.float32).expand(channels,-1,-1)#.to(\"cuda\")\n",
    "y_torch = torch.tensor(y, dtype=torch.float32).expand(channels,-1).unsqueeze(-1)#.to(\"cuda\")\n",
    "w_torch = torch.tensor(w, dtype=torch.float32).expand(channels,-1).unsqueeze(-1)#.to(\"cuda\")\n",
    "\n",
    "print(X_torch.shape, y_torch.shape, w_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648276f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/philiw/tthbb-reweighting/1a8a1aea439f46ae97aa6996431b5ac6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in 'c:\\\\Users\\\\phili\\\\Desktop\\\\Studium\\\\6. Semester\\\\Semesterarbeit\\\\Binary_Classifier_Reweighting' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Comet experiment\n",
    "experiment = Experiment(\n",
    "    api_key=\"s0G6qTP8E4YpcrewlNfry98cY\",\n",
    "    project_name=\"ttHbb-reweighting\",\n",
    "    workspace=\"philiw\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1706d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient implementation of linear layers for ensembles of networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, channels):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.channels = channels\n",
    "        self.weight = nn.Parameter(torch.empty((channels, out_features, in_features)))\n",
    "        self.bias = nn.Parameter(torch.empty((channels, out_features)))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for i in range(self.channels):\n",
    "            torch.nn.init.kaiming_uniform_(self.weight[i], a=math.sqrt(5))\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight[i])\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias[i], -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.baddbmm(self.bias[:,None,:], input, self.weight.transpose(1,2))\n",
    "\n",
    "def kernel(x, y):\n",
    "    \"\"\"\n",
    "    RBF kernel with median estimator\n",
    "    \"\"\"\n",
    "    channels = len(x)\n",
    "    dnorm2 = (x.reshape(channels,1,-1) - y.reshape(1,channels,-1)).square().sum(dim=2)\n",
    "    sigma = torch.quantile(dnorm2.detach(), 0.5) / (2 * math.log(channels + 1))\n",
    "    return torch.exp(- dnorm2 / (2*sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab018aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log hyperparameters\n",
    "experiment.log_parameters({\n",
    "    \"input_dim\": length_features,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"channels\": channels,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"n_epochs\": n_epochs\n",
    "})\n",
    "\n",
    "\n",
    "n_events = len(X)\n",
    "all_idx = np.arange(n_events)\n",
    "\n",
    "idx_train, idx_test = train_test_split(all_idx, test_size=0.5, random_state=42) # Optional: stratify\n",
    "\n",
    "# keep  convenience copy that contains ONLY the EFT rows inside the train fold\n",
    "eft_train_idx = idx_train[y[idx_train] == 0]\n",
    "target_train_idx = idx_train[y[idx_train] == 1]\n",
    "eft_test_idx = idx_test[y[idx_test] == 0]\n",
    "target_test_idx = idx_test[y[idx_test] == 1]\n",
    "\n",
    "X_train = X_torch[:,idx_train]\n",
    "y_train = y_torch[:,idx_train]\n",
    "w_train = w_torch[:,idx_train]\n",
    "\n",
    "X_test = X_torch[:,idx_test]\n",
    "y_test = y_torch[:,idx_test]\n",
    "w_test = w_torch[:,idx_test]\n",
    "\n",
    "# 2. Build train and test datasets\n",
    "train_dataset = TensorDataset(X_train, y_train, w_train)\n",
    "test_dataset  = TensorDataset(X_test,  y_test,  w_test)\n",
    "\n",
    "# 3. Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. (Optional) Log sizes\n",
    "experiment.log_metric(\"train_size\", len(train_dataset))\n",
    "experiment.log_metric(\"test_size\",  len(test_dataset))\n",
    "\n",
    "model = nn.Sequential(\n",
    "    StackedLinear(length_features, hidden_dim, channels),\n",
    "    nn.ReLU(),\n",
    "    StackedLinear(hidden_dim, hidden_dim, channels),\n",
    "    nn.ReLU(),\n",
    "    StackedLinear(hidden_dim, 1, channels),\n",
    ")#.to(\"cuda\")\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1 / (2*len(X)*prior_width**2))\n",
    "#sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, n_epochs*n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c2cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_eft(model, X, y, w):\n",
    "    \"\"\"\n",
    "    X: Tensor of shape (channels, n_events, n_features)\n",
    "    y: Tensor of shape (channels, n_events, 1) — same labels in each channel\n",
    "    w: Tensor of shape (channels, n_events, 1) — same MC weights in each channel\n",
    "\n",
    "    Returns three tensors, each of shape (channels, n_eft_events):\n",
    "      f_eft               — sigmoid(model) on EFT events, per channel\n",
    "      reweighted_wts      — original MC weights * w(x), per channel\n",
    "      reweighting_factors — w(x)=f/(1-f), per channel\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1) remove the singleton label/weight dim -> (channels, n_events)\n",
    "    y_all    = y.squeeze(-1).cpu().numpy()\n",
    "    w_mc_all = w.squeeze(-1).cpu().numpy()\n",
    "\n",
    "    # 2) build mask for EFT events (use channel 0 since labels are identical across channels)\n",
    "    eft_mask = (y_all[0] == 0)    # shape (n_events,)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 3) forward pass: logits (channels, n_events, 1) -> squeeze -> (channels, n_events)\n",
    "        logits = model(X).squeeze(-1)\n",
    "        f_all  = torch.sigmoid(logits).cpu().numpy()             # f(x), shape (channels, n_events)\n",
    "        w_new  = f_all / (1.0 - f_all)              # reweight factors, same shape\n",
    "\n",
    "    # 4) mask off only the EFT events along the event-dimension\n",
    "    f_eft               = f_all[:,    eft_mask]  # (channels, n_eft)\n",
    "    reweighting_factors = w_new[:,    eft_mask]  # (channels, n_eft)\n",
    "    reweighted_wts      = w_mc_all[:, eft_mask] * reweighting_factors\n",
    "\n",
    "    return f_eft, reweighted_wts, reweighting_factors\n",
    "\n",
    "def plot_dist_f(f_eft, eft_indices = None, epoch=-1):\n",
    "    masked_weights_eft = df_combined.weight[eft_indices].to_numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.hist(f_eft, bins=50, weights=masked_weights_eft)\n",
    "    ax.set_xlabel(\"f(x)\")\n",
    "    ax.set_ylabel(\"Weighted Events\")\n",
    "    filename = f\"distribution_f.png\"\n",
    "    #plt.show()  # Optionally, you can omit this in non-interactive scripts\n",
    "    if epoch == -1:\n",
    "        plt.savefig(filename)\n",
    "        experiment.log_image(filename, name=filename)\n",
    "    else:\n",
    "        plt.savefig(filename)\n",
    "        experiment.log_image(filename, name=filename, step=epoch)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_dist_w(reweighting_factors, eft_indices = None, epoch=-1):\n",
    "    masked_weights_eft = df_combined.weight[eft_indices].to_numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.hist(reweighting_factors, bins=50, weights=masked_weights_eft)\n",
    "    ax.set_xlabel(\"w(x)\")\n",
    "    ax.set_ylabel(\"Weighted Events\")\n",
    "    filename = f\"distribution_w.png\"\n",
    "    #plt.show()  # Optionally, you can omit this in non-interactive scripts\n",
    "    if epoch == -1:\n",
    "        plt.savefig(filename)\n",
    "        experiment.log_image(filename, name=filename)\n",
    "    else:\n",
    "        plt.savefig(filename)\n",
    "        experiment.log_image(filename, name=filename, step=epoch)\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_bins_and_scale(object, field, num_bins=50):\n",
    "    if field == 'pt':\n",
    "        if object == 'top' or object == 'antitop' or object == 'higgs':\n",
    "            return np.logspace(1.3, 2.9, num_bins), 'log'\n",
    "        if object == 'lepton_gen' or object == 'lepton_reco':\n",
    "            return np.logspace(1.4, 2.7, num_bins), 'log'\n",
    "    elif field == 'eta':\n",
    "        if object == 'lepton_gen' or object == 'lepton_reco':\n",
    "            return np.linspace(-3, 3, num_bins), 'linear'\n",
    "        else:\n",
    "            return np.linspace(-4, 4, num_bins), 'linear'\n",
    "    elif field == 'phi':\n",
    "        return np.linspace(-np.pi, np.pi, num_bins), 'linear'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown field: {field}\")\n",
    "    \n",
    "def chi2_test(data_counts, model_counts, data_unc=None, model_unc=None):\n",
    "    \"\"\"\n",
    "    Compute a chi^2 statistic comparing two binned histograms.\n",
    "    data_counts: 1D array of bin counts from target data\n",
    "    model_counts: 1D array of bin counts from your model (e.g. reweighted EFT)\n",
    "    \n",
    "    Returns:\n",
    "        chi2_val: the sum of (data - model)^2 / (sigma^2)\n",
    "        ndof: degrees of freedom (naive = number of nonempty bins - 1)\n",
    "        p_value: the p-value for that chi^2 and ndof\n",
    "    \"\"\"\n",
    "    data_counts = np.ravel(data_counts)\n",
    "    model_counts = np.ravel(model_counts)\n",
    "    \n",
    "    # If uncertainties are not provided, assume Poisson errors\n",
    "    if data_unc is None:\n",
    "        data_unc = np.sqrt(data_counts)\n",
    "    else:\n",
    "        data_unc = np.ravel(data_unc)\n",
    "    if model_unc is None:\n",
    "        model_unc = np.sqrt(model_counts)\n",
    "    else:\n",
    "        model_unc = np.ravel(model_unc)\n",
    "\n",
    "    total_unc_sq = data_unc**2 + model_unc**2\n",
    "\n",
    "    mask = total_unc_sq > 0\n",
    "    \n",
    "    chi2_val = np.sum((data_counts[mask] - model_counts[mask])**2 / total_unc_sq[mask])\n",
    "    ndof = np.sum(mask) - 1  # naive degrees of freedom\n",
    "    p_value = 1.0 - chi2.cdf(chi2_val, ndof)\n",
    "    return chi2_val/ndof, ndof, p_value\n",
    "\n",
    "def plot_1d_dist(reweighted_wts, std_devs, eft_indices=None, target_indices = None, epoch=-1): #Default select all events in \n",
    "    metrics = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        for field in fields:\n",
    "\n",
    "            bins, scale = get_bins_and_scale(obj, field)\n",
    "            \n",
    "            for order in orders:\n",
    "                # Extract the appropriate variable from the EFT and Target data, select indices based on what the model evaluated on\n",
    "                eft_variable = getattr(getattr(df_combined, obj), field)[:, order][eft_indices].to_numpy()\n",
    "                target_variable = getattr(getattr(df_combined, obj), field)[:, order][target_indices].to_numpy()\n",
    "                masked_weights_eft = df_combined.weight[eft_indices].to_numpy()\n",
    "                masked_weights_target = df_combined.weight[target_indices].to_numpy()\n",
    "\n",
    "                # Define your histogram bins (you may wish to adjust these per variable)\n",
    "                bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "                # Compute the histograms\n",
    "                hist_eft, _    = np.histogram(eft_variable, bins=bins, weights=masked_weights_eft)\n",
    "                hist_rw_eft, _ = np.histogram(eft_variable, bins=bins, weights=reweighted_wts)\n",
    "                hist_rw_eft_up, _ = np.histogram(eft_variable, bins=bins, weights=reweighted_wts + std_devs)\n",
    "                hist_rw_eft_down, _ = np.histogram(eft_variable, bins=bins, weights=reweighted_wts - std_devs)\n",
    "                hist_target, _ = np.histogram(target_variable, bins=bins, weights=masked_weights_target)\n",
    "\n",
    "                # Calculate errors for the histograms\n",
    "                errors_target_sqrd, _ = np.histogram(target_variable, bins=bins, weights=masked_weights_target**2)\n",
    "                errors_target = np.sqrt(errors_target_sqrd)\n",
    "                errors_eft_sqrd, _ = np.histogram(eft_variable, bins=bins, weights=masked_weights_eft**2)\n",
    "                errors_eft = np.sqrt(errors_eft_sqrd)\n",
    "                errors_rwt = np.abs(hist_rw_eft_up - hist_rw_eft_down)/2\n",
    "\n",
    "\n",
    "                # Create the figure with a top and bottom panel\n",
    "                fig, (ax_top, ax_bot) = plt.subplots(\n",
    "                    2, 1, figsize=(10, 10),\n",
    "                    gridspec_kw={'height_ratios': [5, 2]}\n",
    "                )\n",
    "                \n",
    "                # Top panel: draw the histograms\n",
    "                ax_top.stairs(hist_eft, bins, label=\"EFT data (unweighted)\", fill=False)\n",
    "                ax_top.stairs(hist_rw_eft, bins, label=f\"Reweighted EFT ({order})\", fill=False)\n",
    "                ax_top.stairs(hist_target, bins, label=\"Target data\", fill=False)\n",
    "                ax_top.fill_between(\n",
    "                    bins[:-1], hist_rw_eft_down, hist_rw_eft_up,\n",
    "                    step='post', alpha=0.5, label='Counts Uncertainty'\n",
    "                )\n",
    "                ax_top.set_xscale(scale)\n",
    "                ax_top.set_xlabel(f\"{obj}.{field}\")\n",
    "                ax_top.set_ylabel(\"Weighted events\")\n",
    "                ax_top.set_title(f\"Histogram of {obj}.{field}\\n(reweighting: {order})\")\n",
    "                ax_top.legend()\n",
    "\n",
    "                # Bottom panel: compute and plot the ratios\n",
    "                # Ratio of target to original EFT\n",
    "                ratio_old = np.where(hist_eft != 0, hist_target / hist_eft, np.nan)\n",
    "                # Ratio of target to reweighted EFT\n",
    "                ratio_rw = np.where(hist_rw_eft != 0, hist_target / hist_rw_eft, np.nan)\n",
    "                ratio_rw_up = np.where(hist_rw_eft_up != 0, hist_target / hist_rw_eft_up, np.nan)\n",
    "                ratio_rw_down = np.where(hist_rw_eft_down != 0, hist_target / hist_rw_eft_down, np.nan)\n",
    "\n",
    "                ax_bot.plot(bin_centers, ratio_old, 'o-', label=\"Original ratio\", markersize=3)\n",
    "                ax_bot.plot(bin_centers, ratio_rw, 'o-', label=f\"Reweighted ratio ({order})\", markersize=3)\n",
    "                ax_bot.fill_between(\n",
    "                    bin_centers, ratio_rw_down, ratio_rw_up,\n",
    "                    alpha=0.5, label='Reweight Uncertainty'\n",
    "                )\n",
    "                ax_bot.set_xscale(scale)\n",
    "                ax_bot.axhline(1.0, color='gray', linestyle='--')\n",
    "                ax_bot.set_ylim(0.5, 1.5)\n",
    "                ax_bot.set_ylabel(\"Target / EFT\")\n",
    "                ax_bot.set_xlabel(f\"{obj}.{field}\")\n",
    "                ax_bot.legend(fontsize=10)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save & log figures\n",
    "                filename = f\"1d_reweighting_{obj}_{field}_{order}.png\"\n",
    "                plt.savefig(filename)\n",
    "                if epoch == -1:\n",
    "                    experiment.log_image(filename, name=filename)\n",
    "                else:\n",
    "                    experiment.log_image(filename, name=filename, step=epoch)\n",
    "                plt.close()\n",
    "\n",
    "                if epoch == -1:\n",
    "                    # Run your chi² tests\n",
    "                    chi2_unw, ndof_unw, p_unw = chi2_test(hist_eft,    hist_target, errors_eft,    errors_target)\n",
    "                    chi2_nom, ndof_nom, p_nom = chi2_test(hist_rw_eft, hist_target, errors_rwt,    errors_target)\n",
    "\n",
    "                    # Append this row’s results\n",
    "                    metrics.append({\n",
    "                        \"object\":                obj,\n",
    "                        \"field\":                 field,\n",
    "                        \"order\":                 order,\n",
    "                        \"chi2_unweighted\":       chi2_unw,\n",
    "                        \"ndof_unweighted\":       ndof_unw,\n",
    "                        \"pval_unweighted\":       p_unw,\n",
    "                        \"chi2_reweighted\":       chi2_nom,\n",
    "                        \"ndof_reweighted\":       ndof_nom,\n",
    "                        \"pval_reweighted\":       p_nom,\n",
    "                    })\n",
    "\n",
    "    if epoch == -1:\n",
    "        # After exiting all loops: build a DataFrame and log it\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "        csv_name = \"chi2_test_summary_1d.csv\"\n",
    "        experiment.log_table(\n",
    "            filename=csv_name,\n",
    "            tabular_data=df_metrics,\n",
    "        )\n",
    "\n",
    "def plot_2d_dist(reweighted_wts, std_devs, eft_indices=None, target_indices = None, epoch=-1):\n",
    "    metrics = []\n",
    "    \n",
    "    for i, obj1 in enumerate(objects):\n",
    "        for j, obj2 in enumerate(objects):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            for field1 in fields:\n",
    "                for field2 in fields:\n",
    "                    for order in orders:\n",
    "                        # get bins for the 2D histogram\n",
    "                        bins1, scale1 = get_bins_and_scale(obj1, field1, num_bins=20)\n",
    "                        bins2, scale2 = get_bins_and_scale(obj2, field2, num_bins=20)\n",
    "\n",
    "                        # extract EFT phase‐space coords\n",
    "                        eft_x = getattr(getattr(df_combined, obj1), field1)[:, order][eft_indices].to_numpy()\n",
    "                        eft_y = getattr(getattr(df_combined, obj2), field2)[:, order][eft_indices].to_numpy()\n",
    "                        target_x = getattr(getattr(df_combined, obj1), field1)[:, order][target_indices].to_numpy()\n",
    "                        target_y = getattr(getattr(df_combined, obj2), field2)[:, order][target_indices].to_numpy()\n",
    "                        masked_weights_eft = df_combined.weight[eft_indices]\n",
    "                        masked_weights_target = df_combined.weight[target_indices]\n",
    "                        \n",
    "                        # 5) make figure with one subplot per bin\n",
    "\n",
    "                        fig, axs = plt.subplots(1, 4, figsize=(32, 8))\n",
    "\n",
    "                        h_eft_result = axs[0].hist2d(\n",
    "                            eft_x, eft_y, bins=[bins1, bins2],\n",
    "                            weights=masked_weights_eft.to_numpy(), cmap='inferno'  \n",
    "                        )\n",
    "                        axs[0].set_xscale(scale1)\n",
    "                        axs[0].set_yscale(scale2)\n",
    "                        axs[0].set_title(\"EFT (unweighted)\")\n",
    "                        axs[0].set_xlabel(f\"{obj1}-{field1}\")\n",
    "                        axs[0].set_ylabel(f\"{obj2}-{field2}\")\n",
    "                        plt.colorbar(h_eft_result[3], ax=axs[0])\n",
    "                        \n",
    "                        # Reweighted EFT 2D histogram\n",
    "                        h_rew_result = axs[1].hist2d(\n",
    "                            eft_x, eft_y, bins=[bins1, bins2],\n",
    "                            weights=reweighted_wts, cmap='inferno'\n",
    "                        )\n",
    "                        axs[1].set_xscale(scale1)\n",
    "                        axs[1].set_yscale(scale2)\n",
    "                        axs[1].set_title(\"Reweighted EFT\")\n",
    "                        axs[1].set_xlabel(f\"{obj1}-{field1}\")\n",
    "                        axs[1].set_ylabel(f\"{obj2}-{field2}\")\n",
    "                        plt.colorbar(h_rew_result[3], ax=axs[1])\n",
    "                        \n",
    "                        # Target 2D histogram\n",
    "                        h_target_2d = np.histogram2d(\n",
    "                            target_x, target_y, bins=[bins1, bins2],\n",
    "                            weights=masked_weights_target.to_numpy()\n",
    "                        )[0]\n",
    "                        \n",
    "                        # 2D ratio: target / EFT (original)\n",
    "                        ratio_original = np.where(h_eft_result[0] != 0, h_target_2d / h_eft_result[0], np.nan)\n",
    "                        h3 = axs[2].pcolormesh(\n",
    "                            bins1, \n",
    "                            bins2, \n",
    "                            ratio_original.T,  # note transpose\n",
    "                            cmap='inferno',\n",
    "                            vmin=0.5,\n",
    "                            vmax=1.5,\n",
    "                            shading='auto'\n",
    "                        )\n",
    "                        axs[2].set_xscale(scale1)\n",
    "                        axs[2].set_yscale(scale2)\n",
    "                        axs[2].set_title(\"Ratio: Target / EFT\")\n",
    "                        axs[2].set_xlabel(f\"{obj1}-{field1}\")\n",
    "                        axs[2].set_ylabel(f\"{obj2}-{field2}\")\n",
    "                        plt.colorbar(h3, ax=axs[2])\n",
    "                        \n",
    "                        # 2D ratio: target / EFT (reweighted)\n",
    "                        ratio_rew = np.where(h_rew_result[0] != 0, h_target_2d / h_rew_result[0], np.nan)\n",
    "\n",
    "                        h4 = axs[3].pcolormesh(\n",
    "                            bins1, \n",
    "                            bins2,        \n",
    "                            ratio_rew.T, \n",
    "                            cmap='inferno',\n",
    "                            vmin=0.5,\n",
    "                            vmax=1.5,\n",
    "                            shading='auto'\n",
    "                        )\n",
    "                        axs[3].set_xscale(scale1)\n",
    "                        axs[3].set_yscale(scale2)\n",
    "                        axs[3].set_title(\"Ratio: Target / Reweighted EFT\")\n",
    "                        axs[3].set_xlabel(f\"{obj1}-{field1}\")\n",
    "                        axs[3].set_ylabel(f\"{obj2}-{field2}\")\n",
    "                        plt.colorbar(h4, ax=axs[3])\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        filename = f\"2d_reweighting_{obj1}_{field1}_{obj2}_{field2}.png\"\n",
    "                        plt.savefig(filename)\n",
    "                        experiment.log_image(filename, name=filename)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        if epoch == -1:\n",
    "                    \n",
    "                            h_up_2d = np.histogram2d(\n",
    "                                eft_x, eft_y, bins=[bins1, bins2],\n",
    "                                weights=reweighted_wts + std_devs\n",
    "                            )[0]\n",
    "                            h_down_2d = np.histogram2d(\n",
    "                                eft_x, eft_y, bins=[bins1, bins2],\n",
    "                                weights=reweighted_wts - std_devs\n",
    "                            )[0]\n",
    "\n",
    "                            errors_eft_sqrd = np.histogram2d(eft_x, eft_y, bins=(bins1, bins2), weights=masked_weights_eft**2)[0]\n",
    "                            errors_eft = np.sqrt(errors_eft_sqrd) \n",
    "                            errors_target_sqrd = np.histogram2d(target_x, target_y, bins=(bins1, bins2), weights=masked_weights_target**2)[0]\n",
    "                            errors_target = np.sqrt(errors_target_sqrd)\n",
    "                            errors_rwt = np.abs(h_up_2d - h_down_2d)/2\n",
    "\n",
    "                            # Run your chi² tests\n",
    "                            chi2_unw, ndof_unw, p_unw = chi2_test(h_eft_result[0], h_target_2d, errors_eft,    errors_target)\n",
    "                            chi2_nom, ndof_nom, p_nom = chi2_test(h_rew_result[0], h_target_2d, errors_rwt,    errors_target)\n",
    "\n",
    "                            # Append this row’s results\n",
    "                            metrics.append({\n",
    "                                \"object1\":               obj1,\n",
    "                                \"field1\":                field1,\n",
    "                                \"object2\":               obj2,\n",
    "                                \"field2\":                field2,\n",
    "                                \"order\":                 order,\n",
    "                                \"chi2_unweighted\":       chi2_unw,\n",
    "                                \"ndof_unweighted\":       ndof_unw,\n",
    "                                \"pval_unweighted\":       p_unw,\n",
    "                                \"chi2_reweighted\":       chi2_nom,\n",
    "                                \"ndof_reweighted\":       ndof_nom,\n",
    "                                \"pval_reweighted\":       p_nom,\n",
    "                            })\n",
    "    if epoch == -1:\n",
    "        # After exiting all loops: build a DataFrame and log it\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "        csv_name = \"chi2_test_summary_2d.csv\"\n",
    "        experiment.log_table(\n",
    "            filename=csv_name,\n",
    "            tabular_data=df_metrics,\n",
    "        )\n",
    "\n",
    "def plot_weights_in_1d_phasespace(reweighting_factors, eft_indices=None):\n",
    "    n_bins = 3\n",
    "    quantiles = np.quantile(reweighting_factors, np.linspace(0, 1, n_bins + 1))\n",
    "    reweighting_factors_bins = np.digitize(reweighting_factors, bins=quantiles[1:-1], right=False)\n",
    "    labels = {\n",
    "        i: f\"{quantiles[i]:.2f}–{quantiles[i+1]:.2f}\"\n",
    "        for i in range(n_bins)\n",
    "    }\n",
    "\n",
    "    for i, obj in enumerate(objects):\n",
    "        for field in fields:\n",
    "            for order in orders:\n",
    "                # get bins for the 1D histogram\n",
    "                bins, scale = get_bins_and_scale(obj, field)\n",
    "\n",
    "                # extract EFT phase‐space coords\n",
    "                eft_x = getattr(getattr(df_combined, obj), field)[:, order][eft_indices].to_numpy()\n",
    "                w = df_combined.weight[eft_indices].to_numpy()\n",
    "\n",
    "                # 5) make figure with one subplot per bin\n",
    "                fig, axs = plt.subplots(\n",
    "                    1, n_bins,\n",
    "                    figsize=(4 * n_bins, 4),\n",
    "                    sharex=True, sharey=True\n",
    "                )\n",
    "\n",
    "                # 6) fill each subplot with a weighted histogram\n",
    "                for bin_idx in range(n_bins):\n",
    "                    mask = (reweighting_factors_bins == bin_idx)\n",
    "                    h = axs[bin_idx].hist(\n",
    "                        eft_x[mask],\n",
    "                        bins=bins,\n",
    "                        weights=w[mask],\n",
    "                        histtype='stepfilled',\n",
    "                        label=labels[bin_idx],\n",
    "                        alpha=0.5,\n",
    "                        color='blue'\n",
    "                    )\n",
    "                    axs[bin_idx].set_xscale(scale)\n",
    "                    axs[bin_idx].set_title(labels[bin_idx])\n",
    "                    if bin_idx == 0:\n",
    "                        axs[0].set_xlabel(field)\n",
    "                        axs[0].set_ylabel(\"Weighted events\")\n",
    "\n",
    "                \"\"\"\n",
    "                fig.suptitle(\n",
    "                    f\"{obj}-{field} histograms by w‐quantile\",\n",
    "                )\n",
    "                fig.colorbar(\n",
    "                    h[3], \n",
    "                    ax=axs.ravel().tolist(),\n",
    "                )\"\"\"\n",
    "\n",
    "                filename = f\"1d_phasespace_hist_{obj}_{field}.png\"\n",
    "                plt.savefig(filename)\n",
    "                experiment.log_image(filename, name=filename)\n",
    "                plt.close(fig)\n",
    "\n",
    "def plot_weights_in_2d_phasespace(reweighting_factors, eft_indices=None):\n",
    "    # --- your existing quantile‐binning code ---\n",
    "    n_bins = 3\n",
    "    quantiles = np.quantile(reweighting_factors, np.linspace(0, 1, n_bins + 1))\n",
    "    reweighting_factors_bins = np.digitize(reweighting_factors, bins=quantiles[1:-1], right=False)\n",
    "    labels = {\n",
    "        i: f\"{quantiles[i]:.2f}–{quantiles[i+1]:.2f}\"\n",
    "        for i in range(n_bins)\n",
    "    }\n",
    "\n",
    "    # 3) loop over object/field combinations\n",
    "    for i, obj1 in enumerate(objects):\n",
    "        for j, obj2 in enumerate(objects):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            for field1 in fields:\n",
    "                for field2 in fields:\n",
    "                    for order in orders:\n",
    "                        # get bins for the 2D histogram\n",
    "                        bins1, scale1 = get_bins_and_scale(obj1, field1, num_bins=20)\n",
    "                        bins2, scale2 = get_bins_and_scale(obj2, field2, num_bins=20)\n",
    "\n",
    "                        # extract EFT phase‐space coords\n",
    "                        eft_x = getattr(getattr(df_combined, obj1), field1)[:, order][eft_indices].to_numpy()\n",
    "                        eft_y = getattr(getattr(df_combined, obj2), field2)[:, order][eft_indices].to_numpy()\n",
    "                        w = df_combined.weight[eft_indices].to_numpy()\n",
    "\n",
    "                        # 5) make figure with one subplot per bin\n",
    "                        fig, axs = plt.subplots(\n",
    "                            1, n_bins,\n",
    "                            figsize=(4 * n_bins, 4),\n",
    "                            sharex=True, sharey=True\n",
    "                        )\n",
    "\n",
    "                        # 6) fill each subplot with a weighted 2D histogram\n",
    "                        for bin_idx in range(n_bins):\n",
    "                            mask = (reweighting_factors_bins == bin_idx)\n",
    "                            h = axs[bin_idx].hist2d(\n",
    "                                eft_x[mask], eft_y[mask],\n",
    "                                bins=[bins1, bins2],\n",
    "                                weights=w[mask],\n",
    "                                cmap=plt.cm.viridis\n",
    "                            )\n",
    "                            axs[bin_idx].set_xscale(scale1)\n",
    "                            axs[bin_idx].set_yscale(scale2)\n",
    "                            axs[bin_idx].set_title(labels[bin_idx])\n",
    "                            if bin_idx == 0:\n",
    "                                axs[0].set_xlabel(field1)\n",
    "                                axs[0].set_ylabel(field2)\n",
    "\n",
    "                        \"\"\"\n",
    "                        fig.suptitle(\n",
    "                            f\"{obj1}-{field1} vs {obj2}-{field2} 2D histograms by w‐quantile\",\n",
    "                        )\n",
    "                        \"\"\"\n",
    "                        fig.colorbar(\n",
    "                            h[3], \n",
    "                            ax=axs.ravel().tolist(),\n",
    "                        )\n",
    "\n",
    "                        filename = f\"2d_phasespace_hist_{obj1}_{field1}_{obj2}_{field2}.png\"\n",
    "                        plt.savefig(filename)\n",
    "                        experiment.log_image(filename, name=filename)\n",
    "                        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd832d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(f, reweighted_wts, reweighting_factors):\n",
    "    return np.mean(f, axis=0), np.mean(reweighted_wts, axis=0), np.mean(reweighting_factors, axis=0)\n",
    "def calc_std(f, reweighted_wts, reweighting_factors):\n",
    "    return np.std(f, axis=0), np.std(reweighted_wts, axis=0), np.std(reweighting_factors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a4f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] average loss = 0.6951935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:174: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_old = np.where(hist_eft != 0, hist_target / hist_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:174: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_old = np.where(hist_eft != 0, hist_target / hist_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:176: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw = np.where(hist_rw_eft != 0, hist_target / hist_rw_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:176: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw = np.where(hist_rw_eft != 0, hist_target / hist_rw_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:177: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw_up = np.where(hist_rw_eft_up != 0, hist_target / hist_rw_eft_up, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:177: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw_up = np.where(hist_rw_eft_up != 0, hist_target / hist_rw_eft_up, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:178: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw_down = np.where(hist_rw_eft_down != 0, hist_target / hist_rw_eft_down, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:178: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw_down = np.where(hist_rw_eft_down != 0, hist_target / hist_rw_eft_down, np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] average loss = 0.6943566\n",
      "[Epoch 2] average loss = 0.6938418\n",
      "[Epoch 3] average loss = 0.6935428\n",
      "[Epoch 4] average loss = 0.6933950\n",
      "[Epoch 5] average loss = 0.6933426\n",
      "[Epoch 6] average loss = 0.6933306\n",
      "[Epoch 7] average loss = 0.6933309\n",
      "[Epoch 8] average loss = 0.6933264\n",
      "[Epoch 9] average loss = 0.6933079\n",
      "[Epoch 10] average loss = 0.6932751\n",
      "[Epoch 11] average loss = 0.6932305\n",
      "[Epoch 12] average loss = 0.6931805\n",
      "[Epoch 13] average loss = 0.6931288\n",
      "[Epoch 14] average loss = 0.6930780\n",
      "[Epoch 15] average loss = 0.6930299\n",
      "[Epoch 16] average loss = 0.6929846\n",
      "[Epoch 17] average loss = 0.6929430\n",
      "[Epoch 18] average loss = 0.6929054\n",
      "[Epoch 19] average loss = 0.6928710\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x,y,w in train_loader:\n",
    "\n",
    "        logits = model(x) # (channels, batch_size, 1)\n",
    "\n",
    "        l = loss_fn(logits, y) # (channels, batch_size, 1)\n",
    "        weighted_l = l*w \n",
    "        cl_loss = weighted_l.sum(dim=[-1,-2])/w.sum(dim=[-1,-2])\n",
    "\n",
    "        k = kernel(weighted_l, weighted_l.detach()) # (channels, channels)\n",
    "        kernel_loss = (k.sum(dim=1) / k.detach().sum(dim=1) - 1) / w.sum(dim=[-1,-2])\n",
    "        \n",
    "        #loss = torch.mean(cl_loss, dim=0)\n",
    "        loss = torch.mean(cl_loss + kernel_loss, dim=0) \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #sched.step()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"[Epoch {e}] average loss = {avg_loss:.7f}\")\n",
    "    experiment.log_metric(\"average_loss\", avg_loss, step=e)\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        f_eft, reweighted_wts, reweighting_factors = evaluate_model_on_eft(model, X_train, y_train, w_train)\n",
    "        \n",
    "        mean_f_eft, mean_reweighted_wts, mean_reweighting_factors = calc_mean(f_eft, reweighted_wts, reweighting_factors)\n",
    "        std_f_eft, std_reweighted_wts, std_reweighting_factors = calc_std(f_eft, reweighted_wts, reweighting_factors)\n",
    "\n",
    "        plot_1d_dist(mean_reweighted_wts, std_reweighted_wts, eft_indices=eft_train_idx, target_indices=target_train_idx,epoch=e)\n",
    "        \n",
    "    #f = plt.figure()\n",
    "    #plt.imshow(kernel(l, l.detach()).cpu().detach().numpy())\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:174: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_old = np.where(hist_eft != 0, hist_target / hist_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:174: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_old = np.where(hist_eft != 0, hist_target / hist_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:176: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw = np.where(hist_rw_eft != 0, hist_target / hist_rw_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:176: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw = np.where(hist_rw_eft != 0, hist_target / hist_rw_eft, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:177: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw_up = np.where(hist_rw_eft_up != 0, hist_target / hist_rw_eft_up, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:177: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw_up = np.where(hist_rw_eft_up != 0, hist_target / hist_rw_eft_up, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:178: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_rw_down = np.where(hist_rw_eft_down != 0, hist_target / hist_rw_eft_down, np.nan)\n",
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_17752\\3596319015.py:178: RuntimeWarning: invalid value encountered in divide\n",
      "  ratio_rw_down = np.where(hist_rw_eft_down != 0, hist_target / hist_rw_eft_down, np.nan)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_2d_dist() missing 1 required positional argument: 'std_devs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m plot_dist_w(mean_reweighting_factors, eft_indices\u001b[38;5;241m=\u001b[39meft_test_idx, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m plot_1d_dist(mean_reweighted_wts, std_reweighted_wts, eft_indices\u001b[38;5;241m=\u001b[39meft_test_idx, target_indices\u001b[38;5;241m=\u001b[39mtarget_test_idx, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mplot_2d_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_reweighted_wts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meft_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meft_test_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_test_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m plot_weights_in_1d_phasespace(mean_reweighting_factors, eft_indices\u001b[38;5;241m=\u001b[39meft_test_idx)\n\u001b[0;32m     15\u001b[0m plot_weights_in_2d_phasespace(mean_reweighting_factors, eft_indices\u001b[38;5;241m=\u001b[39meft_test_idx)\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_2d_dist() missing 1 required positional argument: 'std_devs'"
     ]
    }
   ],
   "source": [
    "f, reweighted_wts, reweighting_factors = evaluate_model_on_eft(model, X_test, y_test, w_test)\n",
    "\n",
    "first_f = f[0]\n",
    "first_reweighted_wts = reweighted_wts[0]\n",
    "first_reweighting_factors = reweighting_factors[0]\n",
    "\n",
    "mean_f, mean_reweighted_wts, mean_reweighting_factors = calc_mean(f, reweighted_wts, reweighting_factors)\n",
    "std_f, std_reweighted_wts, std_reweighting_factors = calc_std(f, reweighted_wts, reweighting_factors)\n",
    "\n",
    "plot_dist_f(mean_f, eft_indices=eft_test_idx, epoch=-1)\n",
    "plot_dist_w(mean_reweighting_factors, eft_indices=eft_test_idx, epoch=-1)\n",
    "plot_1d_dist(mean_reweighted_wts, std_reweighted_wts, eft_indices=eft_test_idx, target_indices=target_test_idx, epoch=-1)\n",
    "plot_2d_dist(mean_reweighted_wts, std_reweighted_wts, eft_indices=eft_test_idx, target_indices=target_test_idx, epoch=-1)\n",
    "plot_weights_in_1d_phasespace(mean_reweighting_factors, eft_indices=eft_test_idx)\n",
    "plot_weights_in_2d_phasespace(mean_reweighting_factors, eft_indices=eft_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : thoughtless_abbey_3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/philiw/tthbb-reweighting/279a4c34ca0346d5b9c7f8c1d2474b93\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     average_loss [20] : (0.6928115487098694, 0.6946315765380859)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2]          : (0.6929818391799927, 0.6946315765380859)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_size         : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_size        : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_dim    : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_dim     : 15\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_epochs      : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataframe           : 1 (1.51 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 242\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
